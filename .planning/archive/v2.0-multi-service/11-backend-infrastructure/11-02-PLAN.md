---
phase: 11-backend-infrastructure
plan: 02
type: auto
---

<objective>
Create API routes for Document Intelligence service with processing job management, template operations, and extracted data retrieval. Extend Supabase schema to support document processing data.

Purpose: Provide backend endpoints for the Document Intelligence UI pages and establish database tables for storing templates, processing jobs, and extracted data. This completes the backend infrastructure for the v2.0 multi-service platform.

Output: API routes at `/api/documents/templates`, `/api/documents/jobs`, `/api/documents/data` with GET/POST/PUT/DELETE handlers, mock data, and Supabase schema migrations for documents, templates, extraction_jobs, and extracted_data tables.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-document-intelligence-ui/10-01-SUMMARY.md
@.planning/phases/10-document-intelligence-ui/10-02-SUMMARY.md
@.planning/phases/10-document-intelligence-ui/10-03-SUMMARY.md
@supabase/migrations/001_initial_schema.sql

**Reference patterns:**
- Supabase table structure from existing conversations/messages schema
- Next.js API routes from v1.0 webhook patterns
- Filtering and pagination from Phase 11-01
- Real-time support requires REPLICA IDENTITY FULL for new tables

**Document Intelligence API specifics:**
- `/api/documents/templates` - CRUD for extraction templates
- `/api/documents/jobs` - Processing job management with status tracking
- `/api/documents/data` - Retrieved extracted data from completed jobs

**Schema requirements:**
- documents table: id, file_name, file_type, file_size, storage_path, status, template_id, created_at, updated_at
- templates table: id, name, description, type (invoice, receipt, contract, form, custom), fields (jsonb), status (active/archived), created_by, usage_count, success_rate, created_at, updated_at
- extraction_jobs table: id, document_id, template_id, status (pending, processing, completed, failed), progress, result (jsonb), error_message, started_at, completed_at, created_at
- extracted_data table: id, job_id, field_name, extracted_value, confidence, created_at

**Mock data requirements:**
- 8-10 templates (matching Template Builder page)
- 20-30 extraction jobs with various statuses
- Sample extracted data for completed jobs
- All data should align with Processing Queue and Extracted Data pages

**Architecture decisions from prior phases:**
- Service color: Orange (#F59E0B → #EF4444)
- All authenticated agents see all data (simple RLS)
- Job status tracking: pending → processing → completed/failed
- Soft delete support for templates (archived status)
- Jobs table references existing conversations if needed for document context

**Established patterns:**
- Timestamps in ISO format
- UUID primary keys
- Proper indexing for common queries
- Updated_at trigger function (existing from initial schema)
- RLS policies for authenticated access
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create document processing schema migration and TypeScript types</name>
  <files>supabase/migrations/005_document_intelligence_schema.sql, src/types/documents.ts</files>
  <action>
  1. Create `supabase/migrations/005_document_intelligence_schema.sql` with:

     a) Documents table:
        - id (uuid primary key)
        - file_name (text not null)
        - file_type (text: pdf, docx, jpg, png, etc.)
        - file_size (integer - bytes)
        - storage_path (text - Supabase storage reference)
        - status (text: 'pending', 'processing', 'completed', 'failed')
        - template_id (uuid references templates.id)
        - created_at, updated_at (timestamptz)

     b) Templates table:
        - id (uuid primary key)
        - name (text unique not null)
        - description (text)
        - type (text: 'invoice', 'receipt', 'contract', 'form', 'custom')
        - fields (jsonb - array of field definitions with name, type, required, validation)
        - status (text: 'active', 'archived')
        - created_by (uuid references agents.id)
        - usage_count (integer default 0)
        - success_rate (numeric - percentage 0-100)
        - created_at, updated_at (timestamptz)

     c) ExtractionJobs table:
        - id (uuid primary key)
        - document_id (uuid references documents.id on delete cascade)
        - template_id (uuid references templates.id)
        - status (text: 'pending', 'processing', 'completed', 'failed')
        - progress (integer 0-100)
        - result (jsonb - extracted data in template field structure)
        - error_message (text)
        - started_at (timestamptz)
        - completed_at (timestamptz)
        - created_at, updated_at (timestamptz)

     d) ExtractedData table:
        - id (uuid primary key)
        - job_id (uuid references extraction_jobs.id on delete cascade)
        - field_name (text)
        - extracted_value (text)
        - confidence (numeric 0-1 for accuracy)
        - created_at (timestamptz)

     e) Indexes:
        - idx_documents_template on documents(template_id)
        - idx_documents_status on documents(status)
        - idx_documents_created on documents(created_at desc)
        - idx_templates_name on templates(name)
        - idx_templates_type on templates(type)
        - idx_jobs_document on extraction_jobs(document_id)
        - idx_jobs_template on extraction_jobs(template_id)
        - idx_jobs_status on extraction_jobs(status)
        - idx_jobs_created on extraction_jobs(created_at desc)
        - idx_extracted_data_job on extracted_data(job_id)

     f) Updated_at triggers:
        - Apply update_updated_at() trigger to documents, templates, extraction_jobs tables

     g) RLS Policies:
        - Agents can view all documents (select)
        - Agents can insert/update documents
        - Agents can view all templates
        - Agents can insert/update/delete templates
        - Agents can view all extraction_jobs and extracted_data
        - Agents can insert/update extraction_jobs

     h) Real-time:
        - ALTER TABLE extraction_jobs REPLICA IDENTITY FULL;
        - (for real-time updates on job progress)

  2. Create `src/types/documents.ts` with TypeScript interfaces:
     - Document: id, file_name, file_type, file_size, storage_path, status, template_id, created_at, updated_at
     - DocumentField: name, type ('text'|'number'|'date'|'currency'|'select'|'boolean'), required, validation?, options?
     - Template: id, name, description, type, fields[], status, created_by, usage_count, success_rate, created_at, updated_at
     - ExtractionJob: id, document_id, template_id, status, progress, result, error_message, started_at, completed_at, created_at
     - ExtractedData: id, job_id, field_name, extracted_value, confidence, created_at

  3. Add request/response types:
     - CreateDocumentRequest: file_name, file_type, file_size, template_id
     - CreateTemplateRequest: name, description, type, fields
     - CreateJobRequest: document_id, template_id
     - UpdateJobRequest: status, progress, result?, error_message?

  4. Export all types for use in API routes.
  </action>
  <verify>
  - Migration file is valid SQL with proper syntax
  - Tables created with correct column types and constraints
  - Indexes created for performance
  - RLS policies in place for authenticated users
  - REPLICA IDENTITY FULL set on extraction_jobs
  - TypeScript types compile without errors
  - All types exported for import in other files
  </verify>
  <done>Schema migration created with all required tables, indexes, triggers, RLS policies, and TypeScript types defined</done>
</task>

<task type="auto">
  <name>Task 2: Create document and template API endpoints with mock data</name>
  <files>src/app/api/documents/templates/route.ts, src/app/api/documents/templates/[id]/route.ts, src/app/api/documents/jobs/route.ts, src/app/api/documents/jobs/[id]/route.ts</files>
  <action>
  1. Create `src/app/api/documents/templates/route.ts`:
     - GET: Return all templates with optional filter (?type=invoice, ?status=active)
     - POST: Accept CreateTemplateRequest, return created template with generated ID
     - Include 8-10 mock templates matching Template Builder page

  2. Create `src/app/api/documents/templates/[id]/route.ts`:
     - GET: Return single template by ID
     - PUT: Update template (name, description, fields, status)
     - DELETE: Soft delete (set status to 'archived')

  3. Create `src/app/api/documents/jobs/route.ts`:
     - GET: Return extraction jobs with filtering:
       - ?status=completed - Filter by status
       - ?template_id=uuid - Filter by template
       - ?from_date=YYYY-MM-DD&to_date=YYYY-MM-DD - Date range
       - ?limit=50&offset=0 - Pagination
     - POST: Accept CreateJobRequest, return new job with status 'pending'
     - Default to 50 most recent jobs

  4. Create `src/app/api/documents/jobs/[id]/route.ts`:
     - GET: Return single job with full extracted data
     - PUT: Update job (status, progress, result, error_message)
     - DELETE: Remove job

  5. Mock data for templates (8-10 templates):
     - Invoice: 5 fields (invoice_number, date, total, vendor, items)
     - Receipt: 4 fields (receipt_number, date, total, merchant)
     - Contract: 7 fields (contract_id, parties, date, amount, terms, signatures, expiry)
     - Application Form: 10 fields (name, email, phone, position, experience, education, etc.)
     - 4 custom templates with various field counts
     - All with realistic success_rates (85-98%) and usage_counts (10-500)

  6. Mock data for jobs (20-30 jobs):
     - Mix of statuses: 40% completed, 30% processing, 20% pending, 10% failed
     - Progress: completed=100, processing=20-80, pending=0, failed varies
     - Various templates and documents
     - Dates within last 30 days
     - For completed jobs: full result with extracted field data
     - For failed jobs: error_message like "Unable to extract field: vendor (low confidence)"
     - Confidence scores 0.8-1.0 for successful extractions

  7. For completed jobs, include extracted_data samples showing field-by-field results.

  8. Do NOT use database queries - pure mock data for now. Phase 11 execution will add real DB integration if needed.
  </action>
  <verify>
  - curl http://localhost:3000/api/documents/templates returns 200 with array of 8-10 templates
  - curl "http://localhost:3000/api/documents/templates?type=invoice" filters by type
  - curl http://localhost:3000/api/documents/templates/{id} returns single template
  - curl http://localhost:3000/api/documents/jobs returns 200 with array of jobs
  - curl "http://localhost:3000/api/documents/jobs?status=completed" filters by status
  - curl "http://localhost:3000/api/documents/jobs?limit=10&offset=0" returns paginated results
  - curl http://localhost:3000/api/documents/jobs/{id} returns single job with full extracted data
  - All mock data is realistic and matches UI expectations
  - All timestamps in ISO format
  - No console errors or TypeScript errors
  </verify>
  <done>Template and extraction job API endpoints functional with comprehensive mock data</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `npm run build` succeeds without errors
- [ ] All API endpoints respond correctly with proper status codes
- [ ] Mock data is realistic and matches UI pages expectations
- [ ] Schema migration is valid SQL syntax
- [ ] Filtering and pagination work correctly
- [ ] All timestamps in ISO format
- [ ] TypeScript types properly exported and used
- [ ] No console errors when calling endpoints
- [ ] RLS policies present in migration file
- [ ] Phase 10 is complete (Template Builder page) - verify all 3 plans done
</verification>

<success_criteria>

- Supabase schema migration created with all document intelligence tables
- Document and template API endpoints created and functional
- Extraction job management endpoints with status tracking
- TypeScript types defined for all document entities
- Mock data realistic (templates, jobs, extracted data)
- All HTTP methods (GET/POST/PUT/DELETE) working correctly
- Filtering and pagination implemented
- RLS policies configured for authenticated access
- Ready for backend implementation with real Supabase integration in future phases

## Phase 11 Completion

**Phase 11: Backend Infrastructure is 100% complete (2/2 plans finished):**
- ✓ 11-01: Voice Agents API (agents CRUD, call logs with filtering)
- ✓ 11-02: Document Intelligence API (templates, extraction jobs, schema)

**Project Progress**: Phase 11 completes v2.0 multi-service platform backbone
- All UI pages complete (Phases 8-10)
- All backend API infrastructure complete (Phase 11)
- Ready for: Phase 12 (real database integration) or feature expansion

  </success_criteria>

<output>
After completion, create `.planning/phases/11-backend-infrastructure/11-02-SUMMARY.md`:

# Phase 11 Plan 2: Document Intelligence API & Schema Summary

**Created API routes for document processing with database schema for templates, jobs, and extracted data.**

## Accomplishments

- Designed and created Supabase schema migration with 4 new tables (documents, templates, extraction_jobs, extracted_data)
- Implemented templates CRUD endpoints at `/api/documents/templates` with filtering
- Implemented extraction jobs management at `/api/documents/jobs` with status tracking and pagination
- Defined comprehensive TypeScript types for document intelligence entities
- Created realistic mock data (10 templates, 30 extraction jobs)
- Configured RLS policies and indexes for performance
- Set up REPLICA IDENTITY FULL for real-time job progress tracking

## Files Created/Modified

- `supabase/migrations/005_document_intelligence_schema.sql` - Schema migration for all document tables
- `src/types/documents.ts` - TypeScript types for Document Intelligence service
- `src/app/api/documents/templates/route.ts` - Templates list and create endpoints
- `src/app/api/documents/templates/[id]/route.ts` - Template detail, update, delete endpoints
- `src/app/api/documents/jobs/route.ts` - Extraction jobs list with filtering and pagination
- `src/app/api/documents/jobs/[id]/route.ts` - Job detail and update endpoints

## Decisions Made

- Soft delete for templates (archived status instead of hard delete) for audit trail
- JSONB storage for fields and extraction results for flexibility
- Separate extracted_data table for normalized access to individual field results
- Used mock data only - real database integration can be added in future phase
- Extraction progress tracked 0-100% for real-time job monitoring

## Issues Encountered

None - straightforward schema design following established Supabase patterns.

## Next Phase Readiness

✅ Phase 11 complete. v2.0 multi-service platform infrastructure finished.

All UI pages (Phases 8-10) and all backend APIs (Phase 11) complete. Project ready for:
- Phase 12: Real Supabase integration (replace mock data with actual queries)
- Feature expansion: Advanced document processing, analytics, integrations
- Production deployment and customer onboarding
</output>
